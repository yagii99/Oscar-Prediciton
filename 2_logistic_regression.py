# -*- coding: utf-8 -*-
"""2_Logistic_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WYeppDkfaDUHjltd5BKi8Kl-laa1W3Dt

## Classification using Logistic Regression
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""Using the movies dataset, we are going to predict whether a movie will win an Oscar or not.

We will select the following features to do the classification:

*  title_type
*  genre
*  runtime
*  mpaa_rating
*  imdb_num_votes
*  imdb_rating
*  critics_rating
*  critics_score
*  audience_rating
*  audience_score
*  best_actor_win
*  best_actress_win
*  best_dir_win
*  top200_box

Note that "imdb_rating" and other deatures are now being used to predict our target variable "best_pic_win". We also dropped the "best_pic_nom" feature since we would like to predict the possibility of winning before the awards season so we should not rely on nomination information.

We first build a Dataframe using pandas function pd.read_csv,
and drop the unwanted columns.
"""

#Read csv file and grab the wanted columns:
df = pd.read_csv('movies.csv', index_col='title')
df.drop(['imdb_url', 'rt_url', 'thtr_rel_year', 'thtr_rel_day', 'dvd_rel_year', 'dvd_rel_day'], axis=1, inplace=True)

data = df.loc[:,['title_type', 'genre', 'runtime', 'mpaa_rating', 'imdb_rating', 'imdb_num_votes', 'critics_rating', 'critics_score', 'audience_rating', 'audience_score', 'best_pic_win', 'best_actor_win', 'best_actress_win', 'best_dir_win', 'top200_box']]
data.head()

"""We should see if there is missing data (NULL) and remove them from the dataset"""

data.isna().sum()

#Remove samples with missing data
data.dropna(axis=0, inplace=True)
#Check the number of NULL values in each column
data.isna().sum()

"""Since we have some categorical features, we will now transform them into numerical values using one-hot or dummy encoding."""

#one-hot Encoding
data = pd.get_dummies(data, drop_first=True)
data.head()

"""Since the range of values for each feature are different, we bring all values in the same standing, by scaling them, so that one significant number doesn’t impact the model just because of their large magnitude.

The StandardScaler function will transform our data such that its distribution will have a mean value 0 and standard deviation of 1.
"""

#Scaling 
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

list_num_features = ['runtime', 'imdb_num_votes', 'imdb_rating', 'critics_score', 'audience_score']
data.loc[:,list_num_features] = scaler.fit_transform(data.loc[:,list_num_features])
data.head()

"""We will now split the data for training and testing using train_test_split to get a testing dataset containing 20% of all the values, and the other 80% will be used to train our model.  """

#Split the data into training and testing sets:
from sklearn.model_selection import train_test_split

X = data.drop('best_pic_win_yes', axis=1).values
y = data.loc[:,'best_pic_win_yes'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=51)

print(f"Train size ={X_train.shape[0]}")
print(f"Test size ={X_test.shape[0]}")

"""We will now implement Logistic Regression to solve the classification problem. We can show the accuracy for both the training and testing set, with training having a higher accuracy since this is the data we are using to train our model."""

from sklearn.linear_model import LogisticRegression

logreg=LogisticRegression()

#train a logistic regression model
logreg.fit(X_train, y_train)

#Show accuracy of using the logreg.score() using the test set
print(f'Training accuracy: {logreg.score(X_train, y_train)}')
print(f'Testing accuracy: {logreg.score(X_test, y_test)}')

"""Now we will use the magnitudes of the learned weights to gain insight regarding which variables are more important for the prediction.

First, we access the learned parameters from the model we just trained and display them.
"""

print(f'Model Coefficients: {logreg.coef_}')
print(f'Model y-Intercept ie θ zero: {logreg.intercept_}')

"""Knowing that we scaled our features and therefore parameters with higher magnitudes indicate more important features, we can visualize the parameter magnitude for each feature (for magnitude we use the absolute value of the parameter to make the comparison easier)."""

list_features = list(data.drop('best_pic_win_yes', axis=1).columns)
absolute_params = np.abs(logreg.coef_[0])
plt.figure(figsize=(12,9))
plt.bar(x=list_features, height=absolute_params)
plt.xticks(rotation=90);
plt.xlabel('Features');
plt.ylabel('Model Parameters');
plt.grid()

"""Based on what we see in the above plot, the most important characteristic of a movie that could lead to winning an Oscar, is whether the director had previously won an Oscar in his career.

4) The LogisticRegression class implements by default the L2 norm regularization. Repeat the steps from parts 2 and 3 with L1 (Lasso) regularization.
"""

lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=12);
#Train the model
lasso.fit(X_train, y_train)
#evaluate
print(f'Training accuracy: {lasso.score(X_train, y_train)}')
print(f'Testing accuracy: {lasso.score(X_test, y_test)}')

#Print Coefficients
print(lasso.coef_[0])

#Plot 
absolute_params1 = np.abs(lasso.coef_[0])
plt.figure(figsize=(12,9))
plt.bar(x=list_features, height=absolute_params1)
plt.xticks(rotation=90);
plt.xlabel('Features');
plt.ylabel('Lasso Model Parameters');
plt.grid()

"""5) Compare the resulting Lasso Weights to the weights you got before and comment on the difference.

* Many weights were brought down to zero.
* The weights that are nonzero are the same weights that were found to be of highest importance by the L2 model.
* L1 selects only the most important features.
"""

